{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') \n",
    "from imports import *\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuml\n",
    "from cuml import LogisticRegression as logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_pr_curves(y_true, y_probs, n_bootstraps=1000):\n",
    "    \"\"\"Perform bootstrapping to calculate 95% confidence intervals for ROC and PR curves and plot them\"\"\"\n",
    "    bootstrap_auroc_scores = []\n",
    "    bootstrap_average_precision_scores = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Bootstrap sample (with replacement)\n",
    "        indices = resample(np.arange(len(y_true)), replace=True)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_probs_boot = y_probs[indices]\n",
    "\n",
    "        # Compute metrics for bootstrap sample\n",
    "        bootstrap_auroc_scores.append(roc_auc_score(y_true_boot, y_probs_boot))\n",
    "        bootstrap_average_precision_scores.append(average_precision_score(y_true_boot, y_probs_boot))\n",
    "\n",
    "    # Compute confidence intervals\n",
    "    auroc_lower = np.percentile(bootstrap_auroc_scores, 2.5)\n",
    "    auroc_upper = np.percentile(bootstrap_auroc_scores, 97.5)\n",
    "    ap_lower = np.percentile(bootstrap_average_precision_scores, 2.5)\n",
    "    ap_upper = np.percentile(bootstrap_average_precision_scores, 97.5)\n",
    "\n",
    "    # Calculate original ROC and PR curves\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "    auroc = roc_auc_score(y_true, y_probs)\n",
    "    average_precision = average_precision_score(y_true, y_probs)\n",
    "\n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # ROC Curve\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auroc:.2f})')\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('Receiver Operating Characteristic')\n",
    "    ax1.legend(loc=\"lower right\", title=f'95% CI: [{auroc_lower:.2f}, {auroc_upper:.2f}]')\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    ax2.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {average_precision:.2f})')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curve')\n",
    "    ax2.legend(loc=\"lower left\", title=f'95% CI: [{ap_lower:.2f}, {ap_upper:.2f}]')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Return the confidence intervals\n",
    "    return (np.round(auroc_lower,2), np.round(auroc_upper,2)), (np.round(ap_lower,2), np.round(ap_upper,2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using whole pipeline on GPU using NVIDIA RAPIDS - do later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed saved datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved datasets\n",
    "X_train = np.load('./fashion-mnist-processed-data/train_data.npy')\n",
    "X_val = np.load('./fashion-mnist-processed-data/val_data.npy')\n",
    "X_test = np.load('./fashion-mnist-processed-data/test_data.npy')\n",
    "\n",
    "y_train = np.load('./fashion-mnist-processed-data/train_targets.npy')\n",
    "y_val = np.load('./fashion-mnist-processed-data/val_targets.npy')\n",
    "y_test = np.load('./fashion-mnist-processed-data/test_targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training data\n",
    "n_samples, height, width = X_train.shape\n",
    "X_train = X_train.reshape((n_samples, height*width))\n",
    "\n",
    "# Similarly, reshape the validation and test data \n",
    "n_val_samples, height_val, width_val = X_val.shape\n",
    "X_val = X_val.reshape((n_val_samples, height_val*width_val))\n",
    "\n",
    "n_test_samples, height_test, width_test = X_test.shape\n",
    "X_test = X_test.reshape((n_test_samples, height_test*width_test))\n",
    "\n",
    "# convert to cupy arrays\n",
    "X_train_cp = cp.array(X_train, dtype=cp.float32)\n",
    "y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "X_val_cp = cp.array(X_val, dtype=cp.float32)\n",
    "y_val_cp = cp.array(y_val, dtype=cp.float32)\n",
    "X_test_cp = cp.array(X_test, dtype=cp.float32)\n",
    "y_test_cp = cp.array(y_test, dtype=cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cudf = cudf.DataFrame.from_pandas(pd.DataFrame(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [03:41:06.196077] L-BFGS line search failed (code 3); stopping at the last valid step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the logistic regression model\n",
    "log_reg = logreg(penalty='none', output_type='numpy', max_iter=10000)\n",
    "log_reg.fit(X_train_cp, y_train_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8492\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       600\n",
      "           1       0.95      0.96      0.96       600\n",
      "           2       0.75      0.76      0.75       600\n",
      "           3       0.87      0.89      0.88       600\n",
      "           4       0.74      0.78      0.76       600\n",
      "           5       0.94      0.91      0.93       600\n",
      "           6       0.65      0.59      0.62       600\n",
      "           7       0.93      0.92      0.92       600\n",
      "           8       0.91      0.92      0.92       600\n",
      "           9       0.91      0.95      0.93       600\n",
      "\n",
      "    accuracy                           0.85      6000\n",
      "   macro avg       0.85      0.85      0.85      6000\n",
      "weighted avg       0.85      0.85      0.85      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation set\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_probs = log_reg.predict_proba(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the logistic regression model\n",
    "log_reg_l1 = logreg(penalty='l1', output_type='numpy', max_iter=10000)\n",
    "log_reg_l1.fit(X_train_cp, y_train_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8518\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       600\n",
      "           1       0.96      0.97      0.97       600\n",
      "           2       0.76      0.76      0.76       600\n",
      "           3       0.86      0.90      0.88       600\n",
      "           4       0.76      0.77      0.76       600\n",
      "           5       0.94      0.92      0.93       600\n",
      "           6       0.64      0.60      0.62       600\n",
      "           7       0.91      0.92      0.92       600\n",
      "           8       0.93      0.92      0.93       600\n",
      "           9       0.93      0.94      0.94       600\n",
      "\n",
      "    accuracy                           0.85      6000\n",
      "   macro avg       0.85      0.85      0.85      6000\n",
      "weighted avg       0.85      0.85      0.85      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation set\n",
    "y_val_pred_l1 = log_reg_l1.predict(X_val)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_probs_l1 = log_reg_l1.predict_proba(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred_l1)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred_l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the logistic regression model\n",
    "log_reg_l2 = logreg(penalty='l2', output_type='numpy', tol=1e-2, max_iter=10000)\n",
    "log_reg_l2.fit(X_train_cp, y_train_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8532\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       600\n",
      "           1       0.96      0.96      0.96       600\n",
      "           2       0.75      0.75      0.75       600\n",
      "           3       0.87      0.90      0.89       600\n",
      "           4       0.74      0.78      0.76       600\n",
      "           5       0.94      0.92      0.93       600\n",
      "           6       0.66      0.58      0.62       600\n",
      "           7       0.92      0.92      0.92       600\n",
      "           8       0.93      0.94      0.94       600\n",
      "           9       0.93      0.95      0.94       600\n",
      "\n",
      "    accuracy                           0.85      6000\n",
      "   macro avg       0.85      0.85      0.85      6000\n",
      "weighted avg       0.85      0.85      0.85      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation set\n",
    "y_val_pred_l2 = log_reg_l2.predict(X_val)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_probs_l2 = log_reg_l2.predict_proba(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred_l2)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for logistic regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 0.01 with AUROC: 0.8658\n"
     ]
    }
   ],
   "source": [
    "# Define a set of C values to try\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Initialize variables to store the best score and corresponding C value\n",
    "best_score = 0\n",
    "best_C = None\n",
    "\n",
    "# Perform grid search over the C values\n",
    "for C in C_values:\n",
    "    # Initialize and train the Logistic Regression model with L1 regularization\n",
    "    log_reg_l1 = logreg(penalty='l1', C=C, output_type='numpy', tol=1e-2, max_iter=10000)\n",
    "    log_reg_l1.fit(X_train_cp, y_train_cp)\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred_l1 = log_reg_l1.predict(X_val)\n",
    "    score = accuracy_score(y_val, y_val_pred_l1)\n",
    "\n",
    "    # If the score is better than the best score, update the best score and best C\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_C = C\n",
    "\n",
    "# Output the best C value\n",
    "print(f\"Best C value: {best_C} with AUROC: {best_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model: Logistic Regression with L1 regularization with C=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [03:41:29.059967] QWL-QN stopped, because the line search failed to advance (step delta = 0.000000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new model using the best C value\n",
    "best_log_reg_l1 = logreg(penalty='l1', C=best_C, output_type='numpy', max_iter=10000)\n",
    "best_log_reg_l1.fit(X_train_cp, y_train_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8612\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       600\n",
      "           1       0.97      0.97      0.97       600\n",
      "           2       0.77      0.77      0.77       600\n",
      "           3       0.86      0.91      0.89       600\n",
      "           4       0.76      0.77      0.77       600\n",
      "           5       0.95      0.94      0.94       600\n",
      "           6       0.66      0.61      0.63       600\n",
      "           7       0.92      0.93      0.93       600\n",
      "           8       0.93      0.94      0.94       600\n",
      "           9       0.95      0.95      0.95       600\n",
      "\n",
      "    accuracy                           0.86      6000\n",
      "   macro avg       0.86      0.86      0.86      6000\n",
      "weighted avg       0.86      0.86      0.86      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation set\n",
    "y_val_pred_1 = best_log_reg_l1.predict(X_val)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_probs_1 = best_log_reg_l1.predict_proba(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy_1 = accuracy_score(y_val, y_val_pred_1)\n",
    "print(f\"Validation Accuracy: {val_accuracy_1:.4f}\")\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8424\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81      1000\n",
      "           1       0.97      0.95      0.96      1000\n",
      "           2       0.73      0.75      0.74      1000\n",
      "           3       0.82      0.86      0.84      1000\n",
      "           4       0.73      0.76      0.75      1000\n",
      "           5       0.94      0.92      0.93      1000\n",
      "           6       0.63      0.56      0.59      1000\n",
      "           7       0.92      0.94      0.93      1000\n",
      "           8       0.92      0.93      0.93      1000\n",
      "           9       0.94      0.94      0.94      1000\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_test_pred_l1 = best_log_reg_l1.predict(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "y_test_probs_l1 = best_log_reg_l1.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_l1)\n",
    "print(f\"Validation Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_test, y_test_pred_l1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
